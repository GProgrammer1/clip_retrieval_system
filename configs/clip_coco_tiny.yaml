# CLIP Model Configuration for Tiny COCO Subset (Debugging)

# Dataset paths
data:
  train:
    annotation_file: "images/annotations_trainval2017/annotations/captions_train2017.json"
    image_dir: "images/train2017.1/train2017"
    subset_percentage: 0.01  # Use 1% of training data (very small for debugging)
  val:
    annotation_file: "images/annotations_trainval2017/annotations/captions_val2017.json"
    image_dir: "images/val2017"
    subset_percentage: 0.05  # Use 5% of validation data

# Model architecture
model:
  vision:
    backbone: "resnet50"
    embed_dim: 512
    projection_dim: 256
  text:
    vocab_size: 5000
    embed_dim: 512
    num_layers: 2
    num_heads: 8
    max_seq_length: 77
    projection_dim: 256

# Training
training:
  batch_size: 16
  num_epochs: 5
  learning_rate: 1e-4
  weight_decay: 0.01
  warmup_steps: 100
  gradient_clip_norm: 1.0
  use_amp: true
  
  # Checkpointing
  save_dir: "checkpoints"
  save_every: 1  # Save every epoch
  resume_from: null

# Evaluation
eval:
  batch_size: 32
  top_k: [1, 5, 10]

# Logging
logging:
  log_dir: "logs"
  log_every: 10  # Log more frequently for debugging

