{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Export Embeddings (Notebook Version)\n",
        "\n",
        "This notebook is the notebook version of `scripts/export_embeddings.py` - precompute and save embeddings for faster retrieval.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%pip install torch torchvision numpy pillow pyyaml tqdm scikit-learn transformers\n",
        "\n",
        "# Mount Google Drive if needed\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import yaml\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Add project to path\n",
        "BASE_DIR = Path('/content/CLIP_model') if Path('/content/CLIP_model').exists() else Path.cwd().parent\n",
        "sys.path.insert(0, str(BASE_DIR))\n",
        "\n",
        "from src.data.coco_dataset import build_coco_dataloader\n",
        "from src.models.clip_model import CLIPModel\n",
        "from src.utils.tokenization import SimpleTokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "CONFIG_PATH = BASE_DIR / \"configs/clip_coco_small.yaml\"\n",
        "CHECKPOINT_PATH = BASE_DIR / \"checkpoints/best_model.pt\"\n",
        "OUTPUT_DIR = BASE_DIR / \"embeddings\"\n",
        "\n",
        "# Load config\n",
        "with open(CONFIG_PATH, \"r\") as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "print(f\"Config: {CONFIG_PATH}\")\n",
        "print(f\"Checkpoint: {CHECKPOINT_PATH}\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load model\n",
        "checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n",
        "model = CLIPModel(\n",
        "    vision_config=config[\"model\"][\"vision\"],\n",
        "    text_config=config[\"model\"][\"text\"],\n",
        ").to(device)\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "model.eval()\n",
        "print(\"Model loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build tokenizer\n",
        "tokenizer = SimpleTokenizer(\n",
        "    vocab_size=config[\"model\"][\"text\"][\"vocab_size\"], min_freq=2\n",
        ")\n",
        "\n",
        "temp_loader = build_coco_dataloader(\n",
        "    annotation_file=str(BASE_DIR / config[\"data\"][\"val\"][\"annotation_file\"]),\n",
        "    image_dir=str(BASE_DIR / config[\"data\"][\"val\"][\"image_dir\"]),\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    max_samples=5000,\n",
        ")\n",
        "\n",
        "all_captions = []\n",
        "for batch in tqdm(temp_loader, desc=\"Building vocab\"):\n",
        "    all_captions.extend(batch[\"caption\"])\n",
        "tokenizer.build_vocab(all_captions)\n",
        "print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create data loader\n",
        "def collate_fn(batch, tokenizer, max_seq_length):\n",
        "    \"\"\"Custom collate function.\"\"\"\n",
        "    images = torch.stack([item[\"image\"] for item in batch])\n",
        "    captions = [item[\"caption\"] for item in batch]\n",
        "    image_ids = [item[\"image_id\"] for item in batch]\n",
        "\n",
        "    token_ids = [\n",
        "        tokenizer.encode(cap, max_length=max_seq_length) for cap in captions\n",
        "    ]\n",
        "    token_tensor = torch.tensor(token_ids)\n",
        "    mask = token_tensor == tokenizer.get_pad_token_id()\n",
        "\n",
        "    return {\n",
        "        \"image\": images,\n",
        "        \"text_tokens\": token_tensor,\n",
        "        \"text_mask\": mask,\n",
        "        \"caption\": captions,\n",
        "        \"image_id\": image_ids,\n",
        "    }\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "val_dataset = build_coco_dataloader(\n",
        "    annotation_file=str(BASE_DIR / config[\"data\"][\"val\"][\"annotation_file\"]),\n",
        "    image_dir=str(BASE_DIR / config[\"data\"][\"val\"][\"image_dir\"]),\n",
        "    batch_size=config[\"eval\"][\"batch_size\"],\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    subset_percentage=config[\"data\"][\"val\"].get(\"subset_percentage\"),\n",
        ").dataset\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config[\"eval\"][\"batch_size\"],\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    collate_fn=lambda b: collate_fn(\n",
        "        b, tokenizer, config[\"model\"][\"text\"][\"max_seq_length\"]\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(f\"Validation batches: {len(val_loader)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute embeddings\n",
        "print(\"Computing embeddings...\")\n",
        "image_embeddings = []\n",
        "text_embeddings = []\n",
        "image_ids = []\n",
        "captions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(val_loader, desc=\"Encoding\"):\n",
        "        images = batch[\"image\"].to(device)\n",
        "        text_tokens = batch[\"text_tokens\"].to(device)\n",
        "        text_mask = batch[\"text_mask\"].to(device)\n",
        "\n",
        "        img_emb = model.encode_image(images)\n",
        "        txt_emb = model.encode_text(text_tokens, text_mask)\n",
        "\n",
        "        image_embeddings.append(img_emb.cpu().numpy())\n",
        "        text_embeddings.append(txt_emb.cpu().numpy())\n",
        "        image_ids.extend(batch[\"image_id\"])\n",
        "        captions.extend(batch[\"caption\"])\n",
        "\n",
        "image_embeddings = np.concatenate(image_embeddings, axis=0)\n",
        "text_embeddings = np.concatenate(text_embeddings, axis=0)\n",
        "\n",
        "print(f\"Image embeddings shape: {image_embeddings.shape}\")\n",
        "print(f\"Text embeddings shape: {text_embeddings.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save embeddings\n",
        "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "np.save(OUTPUT_DIR / \"image_embeddings.npy\", image_embeddings)\n",
        "np.save(OUTPUT_DIR / \"text_embeddings.npy\", text_embeddings)\n",
        "\n",
        "metadata = {\n",
        "    \"image_ids\": image_ids,\n",
        "    \"captions\": captions,\n",
        "    \"num_images\": len(image_ids),\n",
        "    \"embedding_dim\": image_embeddings.shape[1],\n",
        "}\n",
        "\n",
        "with open(OUTPUT_DIR / \"metadata.json\", \"w\") as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(f\"\\nEmbeddings saved to {OUTPUT_DIR}\")\n",
        "print(f\"  Image embeddings: {image_embeddings.shape}\")\n",
        "print(f\"  Text embeddings: {text_embeddings.shape}\")\n",
        "print(f\"  Metadata: {len(image_ids)} images\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
