{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Retrieval Script (Notebook Version)\n",
        "\n",
        "This notebook is the notebook version of `scripts/retrieve.py` - search for images or captions using precomputed embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%pip install torch torchvision numpy pillow pyyaml tqdm scikit-learn transformers\n",
        "\n",
        "# Mount Google Drive if needed\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import yaml\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "# Add project to path\n",
        "BASE_DIR = Path('/content/CLIP_model') if Path('/content/CLIP_model').exists() else Path.cwd().parent\n",
        "sys.path.insert(0, str(BASE_DIR))\n",
        "\n",
        "from src.models.clip_model import CLIPModel\n",
        "from src.utils.tokenization import SimpleTokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration - adjust these\n",
        "CONFIG_PATH = BASE_DIR / \"configs/clip_coco_small.yaml\"\n",
        "CHECKPOINT_PATH = BASE_DIR / \"checkpoints/best_model.pt\"\n",
        "EMBEDDINGS_DIR = BASE_DIR / \"embeddings\"\n",
        "TOP_K = 5\n",
        "\n",
        "# Query options - set one of these\n",
        "TEXT_QUERY = \"red sports car drifting on wet road\"  # Set to None to skip text search\n",
        "IMAGE_PATH = None  # Set to image path for image search, e.g., \"images/val2017/000000000139.jpg\"\n",
        "\n",
        "# Load config\n",
        "with open(CONFIG_PATH, \"r\") as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "print(f\"Config: {CONFIG_PATH}\")\n",
        "print(f\"Checkpoint: {CHECKPOINT_PATH}\")\n",
        "print(f\"Embeddings: {EMBEDDINGS_DIR}\")\n",
        "print(f\"Text query: {TEXT_QUERY}\")\n",
        "print(f\"Image path: {IMAGE_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load model\n",
        "checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n",
        "model = CLIPModel(\n",
        "    vision_config=config[\"model\"][\"vision\"],\n",
        "    text_config=config[\"model\"][\"text\"],\n",
        ").to(device)\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "model.eval()\n",
        "print(\"Model loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load embeddings\n",
        "if EMBEDDINGS_DIR.exists():\n",
        "    image_embeddings = np.load(EMBEDDINGS_DIR / \"image_embeddings.npy\")\n",
        "    with open(EMBEDDINGS_DIR / \"metadata.json\", \"r\") as f:\n",
        "        metadata = json.load(f)\n",
        "    print(f\"Loaded {len(metadata['image_ids'])} precomputed embeddings\")\n",
        "else:\n",
        "    print(f\"ERROR: Embeddings not found at {EMBEDDINGS_DIR}\")\n",
        "    print(\"Please run 07_export_embeddings.ipynb first!\")\n",
        "    raise FileNotFoundError(f\"Embeddings directory not found: {EMBEDDINGS_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build tokenizer\n",
        "tokenizer = SimpleTokenizer(\n",
        "    vocab_size=config[\"model\"][\"text\"][\"vocab_size\"], min_freq=2\n",
        ")\n",
        "tokenizer.build_vocab(metadata[\"captions\"])\n",
        "print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Text-to-image retrieval\n",
        "if TEXT_QUERY:\n",
        "    print(f\"\\nQuery: '{TEXT_QUERY}'\")\n",
        "    token_ids = tokenizer.encode(\n",
        "        TEXT_QUERY, max_length=config[\"model\"][\"text\"][\"max_seq_length\"]\n",
        "    )\n",
        "    token_tensor = torch.tensor([token_ids]).to(device)\n",
        "    mask = token_tensor == tokenizer.get_pad_token_id()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        query_embedding = model.encode_text(token_tensor, mask).cpu().numpy()\n",
        "\n",
        "    similarities = np.dot(query_embedding, image_embeddings.T).squeeze()\n",
        "    top_k_indices = np.argsort(similarities)[-TOP_K:][::-1]\n",
        "\n",
        "    print(f\"\\nTop {TOP_K} results:\")\n",
        "    for i, idx in enumerate(top_k_indices, 1):\n",
        "        print(f\"{i}. Image ID: {metadata['image_ids'][idx]}\")\n",
        "        print(f\"   Score: {similarities[idx]:.4f}\")\n",
        "        print(f\"   Caption: {metadata['captions'][idx]}\")\n",
        "else:\n",
        "    print(\"Skipping text-to-image search (TEXT_QUERY is None)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Image-to-text retrieval\n",
        "if IMAGE_PATH:\n",
        "    image_path = BASE_DIR / IMAGE_PATH\n",
        "    print(f\"\\nQuery Image: {image_path}\")\n",
        "    \n",
        "    if not image_path.exists():\n",
        "        print(f\"ERROR: Image not found: {image_path}\")\n",
        "    else:\n",
        "        img = Image.open(image_path).convert(\"RGB\")\n",
        "        img_transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize((224, 224)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(\n",
        "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        "                ),\n",
        "            ]\n",
        "        )\n",
        "        img_tensor = img_transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            image_embedding = model.encode_image(img_tensor).cpu().numpy()\n",
        "\n",
        "        similarities = np.dot(image_embedding, image_embeddings.T).squeeze()\n",
        "        top_k_indices = np.argsort(similarities)[-TOP_K:][::-1]\n",
        "\n",
        "        print(f\"\\nTop {TOP_K} similar images:\")\n",
        "        for i, idx in enumerate(top_k_indices, 1):\n",
        "            print(f\"{i}. Image ID: {metadata['image_ids'][idx]}\")\n",
        "            print(f\"   Score: {similarities[idx]:.4f}\")\n",
        "            print(f\"   Caption: {metadata['captions'][idx]}\")\n",
        "else:\n",
        "    print(\"Skipping image-to-text search (IMAGE_PATH is None)\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
